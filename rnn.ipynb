{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101af4d0",
   "metadata": {},
   "source": [
    "#准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c29a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa0660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"../../data/jaychou_lyrics.txt.zip\")as zin:\n",
    "    with zin.open(\"jaychou_lyrics.txt\") as f:\n",
    "        lyrics=f.read().decode(\"utf-8\")\n",
    "lyrics=lyrics.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "lyrics=lyrics[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a22cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set=list(set(lyrics))\n",
    "vocab_size=len(char_set)\n",
    "char_indices=dict([(char,i)for i,char in enumerate(char_set)])\n",
    "vocab_indices=[char_indices[char] for char in lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f744327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def Dataset_iter(num_steps,batch_size,data,mode=None):\n",
    "    def get_data(pos,num_steps):\n",
    "        return data[pos:pos+num_steps]\n",
    "    num_data=len(data)\n",
    "    num_samples=(num_data-1)//num_steps\n",
    "    num_batch=num_samples//batch_size\n",
    "    example_list=list(range(num_samples))\n",
    "    if mode==\"nearest\":#相邻采样\n",
    "        indices=np.array(example_list[0:num_batch*batch_size]).reshape(batch_size,num_batch)\n",
    "        for i in range(num_batch):\n",
    "            x=[get_data(idx*num_steps,num_steps)for idx in indices[:,i]]\n",
    "            y=[get_data(idx*num_steps+1,num_steps)for idx in indices[:,i]]\n",
    "            yield torch.from_numpy(np.array(x)),torch.from_numpy(np.array(y))        \n",
    "    if mode==\"random\":#随机采样\n",
    "        random.shuffle(example_list) \n",
    "        for epoch in range(num_batch):\n",
    "            batch_pos=batch_size*epoch\n",
    "            batch_indices=example_list[batch_pos:batch_pos+batch_size]\n",
    "            x=[get_data(idx*num_steps,num_steps)for idx in batch_indices]\n",
    "            y=[get_data(idx*num_steps+1,num_steps)for idx in batch_indices]\n",
    "            yield torch.from_numpy(np.array(x)),torch.from_numpy(np.array(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ca61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x,length,dtype=torch.float32):\n",
    "    x=x.long()\n",
    "    zeros=torch.zeros((1,length),dtype=torch.float32,device=device)\n",
    "    zeros[0,x]=1\n",
    "    return zeros\n",
    "def to_onehot(x,length,dtype=torch.float32):\n",
    "    batch_size,num_steps=x.shape\n",
    "    x=x.T\n",
    "    res=[]\n",
    "    for row in x:\n",
    "        for t in row:\n",
    "            res.append(one_hot(x,length,dtype=dtype))\n",
    "    return torch.cat(res).reshape(num_steps,batch_size,length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2a670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class rnn_model(nn.Module):\n",
    "    def __init__(self,hidden_size,vocab_size,num_steps,*args):\n",
    "        super(rnn_model,self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.vocab_size=vocab_size\n",
    "        \n",
    "        #此处h->vocab_size x hidden_size\n",
    "        #RNN默认输入数据的格式是num_steps x batch_size x vocab_size\n",
    "        #每次计算时取\n",
    "        self.rnn=nn.RNN(input_size=vocab_size,hidden_size=hidden_size)\n",
    "        self.state=None\n",
    "        self.linear=nn.Linear(self.hidden_size,self.vocab_size)\n",
    "    def forward(self,inputs,state):\n",
    "        Y,self.state=self.rnn(inputs,state)\n",
    "        Y=self.linear(Y.view(-1,Y.shape[2]))\n",
    "        return Y,self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7979db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()#.item从具有一个元素的张量中取出元素值。\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49814553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_prediction(prefix,num_chars,model,vocab_size,device,char_set,char_indices):\n",
    "    state=None\n",
    "    output=[char_indices[prefix[0]]]\n",
    "    for t in range(len(prefix)+num_chars):\n",
    "        X=one_hot(torch.tensor([output[-1]]),vocab_size)\n",
    "        if state is not None:\n",
    "            if isinstance(state,tuple):\n",
    "                state=(state[0].to(device),state[1].to(device))\n",
    "            else:\n",
    "                state=state.to(device)\n",
    "        X=X.reshape(1,X.shape[0],X.shape[1])\n",
    "        Y,state=model(X,state)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_indices[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y.argmax(dim=1).item()))\n",
    "    return ''.join([char_set[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6aa7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix,num_chars,params,state,rnn,num_hiddens,vocab_size,char_indices,char_set):\n",
    "    output=[char_indices[prefix[0]]]\n",
    "    params=get_params()\n",
    "    for t in range(num_chars+len(prefix)):\n",
    "        params=get_params()\n",
    "        X=one_hot(torch.tensor([output[0]]),vocab_size)\n",
    "        Y,state=rnn(x,state,params)\n",
    "        if t<len(prefix)-1:\n",
    "            output.append(char_indices[prefix[t+1]])\n",
    "        else:\n",
    "            output.append(torch.argmax(Y[0]).item())\n",
    "    return ''.join([char_set[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39fbcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch.optim as optim\n",
    "def train_and_predict_rnn(epoches,rnn_model,batch_size,num_steps,hidden_size,mode,theta,\n",
    "                          device,vocab_indices,vocab_size,char_indices,char_set,prefixes,lr,momentum,num_chars):\n",
    "    rnn=rnn_model(hidden_size,vocab_size,num_steps).to(device)\n",
    "    state=None\n",
    "    Loss=nn.CrossEntropyLoss()\n",
    "    optimizer=optim.Adam(rnn.parameters(),lr=lr,weight_decay=0.0001)\n",
    "    \n",
    "    for epoch in range(1,epoches+1):\n",
    "        start=time.time()\n",
    "        data_iter=Dataset_iter(num_steps,batch_size,vocab_indices,mode)\n",
    "        sum_loss=0.0\n",
    "        n=0.0\n",
    "        for x,y in data_iter:\n",
    "            \n",
    "            if state is not None:\n",
    "                # 使用detach函数从计算图分离隐藏状态, 这是为了\n",
    "                # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\n",
    "                if isinstance (state, tuple): # LSTM, state:(h, c)  \n",
    "                    state = (state[0].detach(), state[1].detach())\n",
    "                else:   \n",
    "                    state = state.detach()\n",
    "            optimizer.zero_grad()\n",
    "            x=to_onehot(x,vocab_size)\n",
    "            \n",
    "            #当state==None时，此时会生成返回初始的state\n",
    "            output,state=rnn(x.to(device),state)\n",
    "            y=torch.transpose(y,0,1).contiguous().view(-1).to(device)\n",
    "            \n",
    "            #交叉熵损失得到的loss默认是平均的loss\n",
    "            loss=Loss(output,y.long())\n",
    "            #赋予参数各自的梯度\n",
    "            loss.backward()\n",
    "            #进行梯度裁剪（防止出现梯度爆炸）\n",
    "            grad_clipping(rnn.parameters(), theta, device)\n",
    "            optimizer.step()\n",
    "            sum_loss+=loss.item()*y.shape[0]\n",
    "            n+=y.shape[0]\n",
    "           \n",
    "        try:\n",
    "            perplexity = math.exp(sum_loss/ n)\n",
    "        except OverflowError:\n",
    "            perplexity = float('inf')\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, perplexity, time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -',rnn_prediction(prefix,num_chars,rnn,vocab_size,device,char_set,char_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca11e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 31.472763, time 2.63 sec\n",
      " - 分开    话 所火  的的     武  在的  度  上  的火  说  的雕     快  切   \n",
      " - 不分开   上  在木     身    火 快      我上 在的    事   的火  快      \n",
      "epoch 100, perplexity 24.127290, time 2.58 sec\n",
      " - 分开      我眼  的我时防七涌 泡的茶  在加的杂 加油学人  的脚  说擦眼 的剔棍忍唱着滴一永头\n",
      " - 不分开  的带 生 完板武  切的的   不惯忠像生切箱牵忆  对起       当的  我吸 现对 更一 \n",
      "epoch 150, perplexity 25.837876, time 2.58 sec\n",
      " - 分开   不   念地脏感壶买轻引脚门擦休 诗窗窗油匠蒙阻滚买  波波擦壶 地度拳打脏现感元哈轻雨 今  \n",
      " - 不分开   忧    板诗装苛怀考问 做仗 尘部 哪 脚 装幅落落念 度 脏刻阻南 的  得欣  念弥再停 \n",
      "epoch 200, perplexity 38.771297, time 2.58 sec\n",
      " - 分开    的 国武      的     漆着不     的 演 一          哮 望     \n",
      " - 不分开       又 跟 天的           河的                望的      形\n",
      "epoch 250, perplexity 49.150232, time 2.58 sec\n",
      " - 分开                           不               道      烛\n",
      " - 不分开                                              式    \n",
      "epoch 300, perplexity 56.602271, time 2.59 sec\n",
      " - 分开                                      切岁     坏     \n",
      " - 不分开                                           膀       \n",
      "epoch 350, perplexity 70.061805, time 2.57 sec\n",
      " - 分开                                                   \n",
      " - 不分开                                                   \n",
      "epoch 400, perplexity 63.407249, time 2.58 sec\n",
      " - 分开                                                   \n",
      " - 不分开                                                   \n",
      "epoch 450, perplexity 81.306777, time 2.61 sec\n",
      " - 分开 么                                                 \n",
      " - 不分开                    么                              \n",
      "epoch 500, perplexity 88.073566, time 2.59 sec\n",
      " - 分开                                          么        \n",
      " - 不分开                                                   \n",
      "epoch 550, perplexity 93.797507, time 2.59 sec\n",
      " - 分开                                                   \n",
      " - 不分开                                                   \n"
     ]
    }
   ],
   "source": [
    "##seting superparameters\n",
    "epoches=1000\n",
    "batch_size=2\n",
    "hidden_size=256\n",
    "lr=1e-3\n",
    "momentum=0.9\n",
    "wight_decay=1e-2\n",
    "mode=\"nearest\"\n",
    "theta=1e-2\n",
    "prefixes=[\"分开\",\"不分开\"]\n",
    "num_steps=32\n",
    "num_chars=50\n",
    "train_and_predict_rnn(epoches,rnn_model,batch_size,num_steps,hidden_size,mode,theta,\n",
    "                          device,vocab_indices,vocab_size,char_indices,char_set,prefixes,lr,momentum,num_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721a6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96227b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
