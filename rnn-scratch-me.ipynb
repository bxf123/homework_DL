{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c884cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc898ba",
   "metadata": {},
   "source": [
    "读取zipfile文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acff9436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63282\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../../data/jaychou_lyrics.txt.zip\")as zin:\n",
    "    with zin.open(\"jaychou_lyrics.txt\") as f:\n",
    "        lyrics=f.read().decode(\"utf-8\")\n",
    "lyrics=lyrics.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "print(len(lyrics))\n",
    "lyrics=lyrics[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46232e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set=list(set(lyrics))\n",
    "vocab_size=len(char_set)\n",
    "char_indices=dict([(char,i)for i,char in enumerate(char_set)])\n",
    "vocab_indices=[char_indices[char] for char in lyrics]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a6b4f",
   "metadata": {},
   "source": [
    "采集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd36e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def Dataset_iter(num_steps,batch_size,data,mode=None):\n",
    "    def get_data(pos,num_steps):\n",
    "        return data[pos:pos+num_steps]\n",
    "    num_data=len(data)\n",
    "    num_samples=(num_data-1)//num_steps\n",
    "    num_batch=num_samples//batch_size\n",
    "    example_list=list(range(num_samples))\n",
    "    if mode==\"nearest\":#相邻采样\n",
    "        indices=np.array(example_list[0:num_batch*batch_size]).reshape(batch_size,num_batch)\n",
    "        for i in range(num_batch):\n",
    "            x=[get_data(idx*num_steps,num_steps)for idx in indices[:,i]]\n",
    "            y=[get_data(idx*num_steps+1,num_steps)for idx in indices[:,i]]\n",
    "            yield torch.from_numpy(np.array(x)),torch.from_numpy(np.array(y))        \n",
    "    if mode==\"random\":#随机采样\n",
    "        random.shuffle(example_list) \n",
    "        for epoch in range(num_batch):\n",
    "            batch_pos=batch_size*epoch\n",
    "            batch_indices=example_list[batch_pos:batch_pos+batch_size]\n",
    "            x=[get_data(idx*num_steps,num_steps)for idx in batch_indices]\n",
    "            y=[get_data(idx*num_steps+1,num_steps)for idx in batch_indices]\n",
    "            yield torch.from_numpy(np.array(x)),torch.from_numpy(np.array(y)) \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0ac52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将采样得到的数据表示成one-hot形式\n",
    "#(num_batch,num_steps)\n",
    "#变成：(num_steps,(num_batch,one-hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4ef8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x,length,dtype=torch.float32):\n",
    "    x=x.long()\n",
    "    zeros=torch.zeros((1,length),dtype=torch.float32,device=device)\n",
    "    zeros[0,x]=1\n",
    "    return zeros\n",
    "def to_onehot(x,length,dtype=torch.float32):\n",
    "    batch_size,num_steps=x.shape\n",
    "    x=x.T\n",
    "    res=[]\n",
    "    for row in x:\n",
    "        for t in row:\n",
    "            res.append(one_hot(x,length,dtype=dtype))\n",
    "    return torch.cat(res).reshape(num_steps,batch_size,length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275571f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1027\n"
     ]
    }
   ],
   "source": [
    "#初始化模型参数\n",
    "num_inputs,num_hiddens,num_outputs=vocab_size,256,vocab_size\n",
    "print(vocab_size)\n",
    "import torch.nn as nn\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7473c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return torch.zeros((batch_size, num_hiddens), device=device)\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H= state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a411eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_steps=5\n",
    "# batch_size=2\n",
    "\n",
    "# for step ,data in enumerate (Dataset_iter(num_steps,batch_size,vocab_indices,\"random\")):\n",
    "#     inputs=to_onehot(data[0],vocab_size).to(device)\n",
    "#     label=to_onehot(data[1],vocab_size).to(device)\n",
    "#     state=init_rnn_state(batch_size, num_hiddens, device)\n",
    "#     params=get_params()\n",
    "#     output,h,=rnn(inputs,state,params)\n",
    "# # print(len(output),output[0].shape,h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6affa615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义预测函数\n",
    "def predict_rnn(prefix,num_chars,params,state,rnn,num_hiddens,vocab_size,char_indices,char_set):\n",
    "    output=[char_indices[prefix[0]]]\n",
    "    params=get_params()\n",
    "    for t in range(num_chars+len(prefix)):\n",
    "        params=get_params()\n",
    "        x=one_hot(torch.tensor([output[0]]),vocab_size)\n",
    "        Y,state=rnn(x,state,params)\n",
    "        if t<len(prefix)-1:\n",
    "            output.append(char_indices[prefix[t+1]])\n",
    "        else:\n",
    "            output.append(torch.argmax(Y[0]).item())\n",
    "    return ''.join([char_set[i] for i in output])\n",
    "# params=get_params()                   \n",
    "# res=predict_rnn(\"分开\",10,params,init_rnn_state,rnn,num_hiddens,vocab_size,char_indices,char_set)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c7ff3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度剪裁\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()#.item从具有一个元素的张量中取出元素值。\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f244a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params,batch_size,lr,epoch):\n",
    "    with torch.no_grad():\n",
    "        if epoch>100:\n",
    "            lr=0.1*lr\n",
    "        for param in params:\n",
    "            param-=lr*param.grad.data/batch_size\n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129e4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19d80612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_and_predict_rnn(epoches,get_params,init_rnn_state,rnn,batch_size,num_steps,num_hiddens,mode,theta,\n",
    "                          device,vocab_indices,length,num_chars,vocab_size,char_indices,char_set,prefixes):\n",
    "    loss=nn.CrossEntropyLoss()\n",
    "    params=get_params()\n",
    "    optimizer=torch.optim.Adam(params,lr=lr)\n",
    "    for epoch in range(1,epoches+1):\n",
    "        if mode!=\"random\":\n",
    "            state=init_rnn_state(batch_size, num_hiddens, device)\n",
    "        data_iter=Dataset_iter(num_steps,batch_size,vocab_indices,mode)\n",
    "        sum_loss=0.0\n",
    "        n=0\n",
    "        for x,y in data_iter:\n",
    "            if mode==\"random\":\n",
    "                state=init_rnn_state(batch_size, num_hiddens, device)\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            optimizer.zero_grad()\n",
    "            x=to_onehot(x,length,torch.float32)\n",
    "            output,state=rnn(x,state,params)\n",
    "            output=torch.cat(output,0)\n",
    "            y=torch.transpose(y,0,1).contiguous().view(-1).to(device)\n",
    "            Loss=loss(output,y.long())#交叉熵损失得到的loss是平均的loss\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()#zero_表示对值进行赋零值\n",
    "            Loss.backward()#赋予参数各自的梯度\n",
    "            grad_clipping(params, theta, device)\n",
    "            optimizer.step()\n",
    "            sum_loss+=Loss.item()*y.shape[0]\n",
    "            n+=y.shape[0]\n",
    "        if (epoch ) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f' % (\n",
    "                epoch , math.exp(sum_loss/ n)))\n",
    "            for prefix in prefixes:\n",
    "                print(' -',predict_rnn(prefix,num_chars,params,state,rnn,num_hiddens,vocab_size,char_indices,char_set))\n",
    "\n",
    "            \n",
    "        print(f\"{epoch:03d}/{epoches}  loss:{math.exp(sum_loss/n):.5f} \")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf922fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001/250  loss:441.97344 \n",
      "002/250  loss:310.58600 \n",
      "003/250  loss:307.20501 \n",
      "004/250  loss:304.40485 \n",
      "005/250  loss:301.87610 \n",
      "006/250  loss:302.05257 \n",
      "007/250  loss:297.30244 \n",
      "008/250  loss:294.11009 \n",
      "009/250  loss:298.31595 \n",
      "010/250  loss:296.63717 \n",
      "011/250  loss:297.24218 \n",
      "012/250  loss:292.55373 \n",
      "013/250  loss:293.25268 \n",
      "014/250  loss:296.66945 \n",
      "015/250  loss:293.98364 \n",
      "016/250  loss:290.89277 \n",
      "017/250  loss:289.82135 \n",
      "018/250  loss:292.16861 \n",
      "019/250  loss:290.22875 \n",
      "020/250  loss:290.11592 \n",
      "021/250  loss:285.70855 \n",
      "022/250  loss:289.30604 \n",
      "023/250  loss:290.78913 \n",
      "024/250  loss:287.22101 \n",
      "025/250  loss:290.07637 \n",
      "026/250  loss:286.25785 \n",
      "027/250  loss:288.45658 \n",
      "028/250  loss:287.94189 \n",
      "029/250  loss:287.26532 \n",
      "epoch 30, perplexity 292.466579\n",
      " - 分开苍我甜腿强掉我窝宇抄奏晶问词抱典硬城币乡承\n",
      " - 不分开怀唱凝都千灌蜡拳始向者狗寻准只形攻较藏蒙这\n",
      "030/250  loss:292.46658 \n",
      "031/250  loss:283.85787 \n",
      "032/250  loss:287.86840 \n",
      "033/250  loss:285.96110 \n",
      "034/250  loss:287.09496 \n",
      "035/250  loss:285.52128 \n",
      "036/250  loss:282.87850 \n",
      "037/250  loss:287.74822 \n",
      "038/250  loss:281.96370 \n",
      "039/250  loss:282.86016 \n",
      "040/250  loss:282.01416 \n",
      "041/250  loss:282.95131 \n",
      "042/250  loss:283.28191 \n",
      "043/250  loss:283.30432 \n",
      "044/250  loss:282.18972 \n",
      "045/250  loss:284.76304 \n",
      "046/250  loss:283.21429 \n",
      "047/250  loss:285.90887 \n",
      "048/250  loss:281.95750 \n",
      "049/250  loss:282.62154 \n",
      "050/250  loss:282.24181 \n",
      "051/250  loss:279.05175 \n",
      "052/250  loss:283.99384 \n",
      "053/250  loss:281.41613 \n",
      "054/250  loss:281.10448 \n",
      "055/250  loss:281.18936 \n",
      "056/250  loss:282.01524 \n",
      "057/250  loss:278.80984 \n",
      "058/250  loss:280.99984 \n",
      "059/250  loss:280.44505 \n",
      "epoch 60, perplexity 283.319372\n",
      " - 分开B讽说廉肉稀市细爽恩螂步九卷如布饿细壁哪做\n",
      " - 不分开凶实何拳相抽较喜内脉的出汹文欠身谁飞担使A\n",
      "060/250  loss:283.31937 \n",
      "061/250  loss:282.38328 \n",
      "062/250  loss:277.28031 \n",
      "063/250  loss:280.83889 \n",
      "064/250  loss:280.23509 \n",
      "065/250  loss:280.98611 \n",
      "066/250  loss:279.87790 \n",
      "067/250  loss:278.24301 \n",
      "068/250  loss:280.62444 \n",
      "069/250  loss:280.84032 \n",
      "070/250  loss:277.30341 \n",
      "071/250  loss:282.11548 \n",
      "072/250  loss:281.13432 \n",
      "073/250  loss:276.02020 \n",
      "074/250  loss:277.33035 \n",
      "075/250  loss:281.64272 \n",
      "076/250  loss:279.60050 \n",
      "077/250  loss:275.88148 \n",
      "078/250  loss:276.98631 \n",
      "079/250  loss:280.59430 \n",
      "080/250  loss:280.91614 \n",
      "081/250  loss:275.15774 \n",
      "082/250  loss:282.36984 \n",
      "083/250  loss:283.02372 \n",
      "084/250  loss:279.12127 \n",
      "085/250  loss:277.79252 \n",
      "086/250  loss:274.49189 \n",
      "087/250  loss:274.31737 \n",
      "088/250  loss:278.50666 \n",
      "089/250  loss:280.88350 \n",
      "epoch 90, perplexity 279.350874\n",
      " - 分开垂别囱制琴辈身内夜江蝪编荒司永医旧王共桌属\n",
      " - 不分开?碌被乡不C楔背汹蝪苍气同爸惹者年泥经毛督\n",
      "090/250  loss:279.35087 \n",
      "091/250  loss:278.77520 \n",
      "092/250  loss:278.66099 \n",
      "093/250  loss:276.41730 \n",
      "094/250  loss:279.42157 \n",
      "095/250  loss:274.91085 \n",
      "096/250  loss:274.78911 \n",
      "097/250  loss:276.71304 \n",
      "098/250  loss:271.23156 \n",
      "099/250  loss:277.88545 \n",
      "100/250  loss:278.34803 \n",
      "101/250  loss:275.81778 \n",
      "102/250  loss:278.04068 \n",
      "103/250  loss:276.56291 \n",
      "104/250  loss:275.15002 \n",
      "105/250  loss:278.66992 \n",
      "106/250  loss:271.74259 \n",
      "107/250  loss:272.93504 \n",
      "108/250  loss:273.01109 \n",
      "109/250  loss:274.83323 \n",
      "110/250  loss:273.54018 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a2274cb98ba2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprefixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"分开\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"不分开\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m train_and_predict_rnn(epoches,get_params,init_rnn_state,rnn,batch_size,num_steps,num_hiddens,mode,theta,\n\u001b[1;32m---> 10\u001b[1;33m                           device,vocab_indices,length,num_chars,vocab_size,char_indices,char_set,prefixes)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-5207603b7e40>\u001b[0m in \u001b[0;36mtrain_and_predict_rnn\u001b[1;34m(epoches, get_params, init_rnn_state, rnn, batch_size, num_steps, num_hiddens, mode, theta, device, vocab_indices, length, num_chars, vocab_size, char_indices, char_set, prefixes)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-56a21cc95cac>\u001b[0m in \u001b[0;36mrnn\u001b[1;34m(inputs, state, params)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mH\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_xh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_hh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_hq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    589\u001b[0m                           \u001b[1;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[1;32m--> 591\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#super parameters\n",
    "\n",
    "num_hiddens=600\n",
    "mode=\"random\"\n",
    "epoches, num_steps, batch_size, lr, theta,length = 250, 30, 32, 1e-3,1e-2,vocab_size\n",
    "pred_period=30\n",
    "num_chars=20\n",
    "prefixes=[\"分开\",\"不分开\"]\n",
    "train_and_predict_rnn(epoches,get_params,init_rnn_state,rnn,batch_size,num_steps,num_hiddens,mode,theta,\n",
    "                          device,vocab_indices,length,num_chars,vocab_size,char_indices,char_set,prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf250a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f04b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
